"""
EGO
"""

'''Loading package'''
import pandas as pd
import numpy as np
from itertools import combinations
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from time import strftime
from time import localtime
from time import perf_counter
from time import sleep
from tqdm import tqdm
import math
import scipy.stats as st
import multiprocessing
from itertools import product
import random

random.seed(654321)
np.random.seed(654321)

'''A function that returns the current time'''
def print_time():
    print(strftime("%Y-%m-%d %H:%M:%S", localtime()))
    return


'''A function that returns the percentage error'''
def percentage_error(y, y_predict ):
    error = np.average( abs(np.subtract(y_predict, y) / y))
    return error * 100
EP_scorer = make_scorer(percentage_error, greater_is_better = False)
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
def calculate_uncertainty(training, search, global_list):
    # Define the parameter grid
    param_grid = {
        'svr__C': [1, 10, 30, 60, 80, 100, 150, 200],
        'svr__gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2]
    }

    number_data = round(training.shape[0] * 0.9)
    sub_training_data = training.sample(number_data, replace=False)

    # Extract features and labels from the training set
    X_train = sub_training_data.iloc[:, :-1].values
    y_train = sub_training_data.iloc[:, -1].values

    # Standardize the features and normalize the labels
    scaler_features = StandardScaler()
    scaler_labels = MinMaxScaler()  # Normalize the labels to be in the range [0, 1]

    X_train_scaled = scaler_features.fit_transform(X_train)
    y_train_normalized = scaler_labels.fit_transform(y_train.reshape(-1, 1)).flatten()

    # Create a pipeline with scaling and SVR
    svr = make_pipeline(StandardScaler(), SVR(kernel='rbf'))

    # Initialize GridSearchCV with the scaled data
    grids = GridSearchCV(svr, param_grid, cv=10, scoring=mse_scorer, n_jobs=1)

    # Fit the model on the scaled data
    grids.fit(X_train_scaled, y_train_normalized)

    # Get the best estimator
    best_clf = grids.best_estimator_

    # Scale the test data
    X_test = search.iloc[:, :].values
    X_test_scaled = scaler_features.transform(X_test)

    # Make predictions on the test data
    predict_test_normalized = best_clf.predict(X_test_scaled)

    # Inverse transform the predictions back to original scale
    predict_test = scaler_labels.inverse_transform(predict_test_normalized.reshape(-1, 1)).flatten()

    # Append the predictions to the global list
    global_list.append(predict_test)
    print_time()

def calculate_uncertainty_grainsize(training, search, global_list):
    # Define the parameter grid
    param_grid = {
        'svr__C': [1, 10, 30, 60, 80, 100, 150, 200],
        'svr__gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2]
    }

    number_data = round(training.shape[0] * 0.9)
    sub_training_data = training.sample(number_data, replace=False)

    # Extract features and labels from the training set
    X_train = sub_training_data.iloc[:, :-1].values
    y_train = sub_training_data.iloc[:, -1].values

    # Standardize the features and normalize the labels
    scaler_features = StandardScaler()
    scaler_labels = MinMaxScaler()  # Normalize the labels to be in the range [0, 1]

    X_train_scaled = scaler_features.fit_transform(X_train)
    y_train_normalized = scaler_labels.fit_transform(y_train.reshape(-1, 1)).flatten()

    # Create a pipeline with scaling and SVR
    svr = make_pipeline(StandardScaler(), SVR(kernel='rbf'))

    # Initialize GridSearchCV with the scaled data
    grids = GridSearchCV(svr, param_grid, cv=10, scoring=mse_scorer, n_jobs=1)

    # Fit the model on the scaled data
    grids.fit(X_train_scaled, y_train_normalized)

    # Get the best estimator
    best_clf = grids.best_estimator_

    # Scale the test data
    X_test = search.iloc[:, :].values
    X_test_scaled = scaler_features.transform(X_test)

    # Make predictions on the test data
    predict_test_normalized = best_clf.predict(X_test_scaled)

    # Inverse transform the predictions back to original scale
    predict_test = scaler_labels.inverse_transform(predict_test_normalized.reshape(-1, 1)).flatten()

    # Ensure all predictions are above 27
    predict_test = np.array([max(val, 27) for val in predict_test])

    # Append the predictions to the global list
    global_list.append(predict_test)


def features_calculation():
    pass

def efficiency_function(search_space_prediction_yield_stength,knowledge_prediction_grain_size,search_space_knoledge_yield_stength):
    bench_mark_search_space_prediction_yield_stength = 987
    bench_mark_knowledge_prediction_grain_size = -30*30
    bench_mark_search_space_knoledge_yield_stength = 994

    search_space_prediction_yield_stength = search_space_prediction_yield_stength.describe().iloc[1:3, :]
    search_space_prediction_yield_stength = pd.DataFrame(search_space_prediction_yield_stength.values.T, index=search_space_prediction_yield_stength.columns, columns=search_space_prediction_yield_stength.index)
    average_value_search_space_prediction_yield_stength = search_space_prediction_yield_stength.iloc[:,0]
    standard_deviation_search_space_prediction_yield_stength = search_space_prediction_yield_stength.iloc[:,1]

    knowledge_prediction_grain_size = (-knowledge_prediction_grain_size).describe().iloc[1:3, :]
    knowledge_prediction_grain_size = pd.DataFrame(knowledge_prediction_grain_size.values.T, index=knowledge_prediction_grain_size.columns,columns=knowledge_prediction_grain_size.index)
    average_value_knowledge_prediction_grain_size = knowledge_prediction_grain_size.iloc[:, 0]
    standard_deviation_knowledge_prediction_grain_size = knowledge_prediction_grain_size.iloc[:, 1]

    search_space_knoledge_yield_stength = (search_space_knoledge_yield_stength).describe().iloc[1:3, :]
    search_space_knoledge_yield_stength = pd.DataFrame(search_space_knoledge_yield_stength.values.T,index=search_space_knoledge_yield_stength.columns,columns=search_space_knoledge_yield_stength.index)
    average_value_knowledge_search_space_knoledge_yield_stength = search_space_knoledge_yield_stength.iloc[:, 0]
    standard_deviation_search_space_knoledge_yield_stength = search_space_knoledge_yield_stength.iloc[:, 1]

    #EGO
    z_ego_search_space_prediction_yield_stength = np.true_divide(np.subtract(average_value_search_space_prediction_yield_stength, bench_mark_search_space_prediction_yield_stength), standard_deviation_search_space_prediction_yield_stength)
    ei_ego_search_space_prediction_yield_stength = standard_deviation_search_space_prediction_yield_stength * z_ego_search_space_prediction_yield_stength * st.norm.cdf(z_ego_search_space_prediction_yield_stength) + standard_deviation_search_space_prediction_yield_stength * st.norm.pdf(z_ego_search_space_prediction_yield_stength)

    z_ego_knowledge_prediction_grain_size = np.true_divide(np.subtract(average_value_knowledge_prediction_grain_size, bench_mark_knowledge_prediction_grain_size), standard_deviation_knowledge_prediction_grain_size)
    ei_ego_knowledge_prediction_grain_size = standard_deviation_knowledge_prediction_grain_size * z_ego_knowledge_prediction_grain_size * st.norm.cdf(z_ego_knowledge_prediction_grain_size) + standard_deviation_knowledge_prediction_grain_size * st.norm.pdf(z_ego_knowledge_prediction_grain_size)

    z_ego_search_space_knoledge_yield_stength = np.true_divide(np.subtract(average_value_knowledge_search_space_knoledge_yield_stength, bench_mark_search_space_knoledge_yield_stength), standard_deviation_search_space_knoledge_yield_stength)
    ei_ego_search_space_knoledge_yield_stength = standard_deviation_search_space_knoledge_yield_stength * z_ego_search_space_knoledge_yield_stength * st.norm.cdf(z_ego_search_space_knoledge_yield_stength) + standard_deviation_search_space_knoledge_yield_stength * st.norm.pdf(z_ego_search_space_knoledge_yield_stength)

    #KG
    #z_kg_search_space_prediction_yield_stength = -1*abs(np.true_divide(np.subtract(average_value_search_space_prediction_yield_stength,max(max(average_value_search_space_prediction_yield_stength),bench_mark_search_space_prediction_yield_stength)),standard_deviation_search_space_prediction_yield_stength))
    #ei_kg_search_space_prediction_yield_stength = standard_deviation_search_space_prediction_yield_stength * z_kg_search_space_prediction_yield_stength * st.norm.cdf(z_kg_search_space_prediction_yield_stength) + standard_deviation_search_space_prediction_yield_stength * st.norm.pdf(z_kg_search_space_prediction_yield_stength)

    #z_kg_knowledge_prediction_grain_size = -1 * abs(np.true_divide(np.subtract(average_value_knowledge_prediction_grain_size,max(max(average_value_knowledge_prediction_grain_size),bench_mark_knowledge_prediction_grain_size)),standard_deviation_knowledge_prediction_grain_size))
    #ei_kg_knowledge_prediction_grain_size = standard_deviation_knowledge_prediction_grain_size * z_kg_knowledge_prediction_grain_size * st.norm.cdf(z_kg_knowledge_prediction_grain_size) + standard_deviation_knowledge_prediction_grain_size * st.norm.pdf(z_kg_knowledge_prediction_grain_size)

    #z_kg_search_space_knoledge_yield_stength = -1 * abs(np.true_divide(np.subtract(average_value_knowledge_search_space_knoledge_yield_stength,max(max(average_value_knowledge_search_space_knoledge_yield_stength),bench_mark_search_space_knoledge_yield_stength)),standard_deviation_search_space_knoledge_yield_stength))
    #ei_kg_search_space_knoledge_yield_stength = standard_deviation_search_space_knoledge_yield_stength * z_kg_search_space_knoledge_yield_stength * st.norm.cdf(z_kg_search_space_knoledge_yield_stength) + standard_deviation_search_space_knoledge_yield_stength * st.norm.pdf(z_kg_search_space_knoledge_yield_stength)

    #Max_P
    #max_p = st.norm.cdf(z_ego,average_value,standard_deviation)


    #ei_all  = ei_ego_search_space_prediction_yield_stength.mul(ei_ego_knowledge_prediction_grain_size).mul(ei_ego_search_space_knoledge_yield_stength)
    #ei = search_space_prediction_yield_stength
    #ei['ei_all'] = ei_all

    ei_all  = ei_ego_search_space_prediction_yield_stength.mul(ei_ego_knowledge_prediction_grain_size).mul(ei_ego_search_space_knoledge_yield_stength)
    ei = search_space_prediction_yield_stength
    ei['ei_all'] = ei_all

    #ei['ei_kg'] = ei_kg
    #ei['max_p'] = max_p
    print_time()
    return ei


def grain_size(training_data):
    # 定义阈值
    threshold = 1083

    d_solution = np.where(
        training_data['STT'] < threshold,
        # 计算 d_Sub_solution
        (training_data['IGS'] ** 2.15 + 1.027584834 * (10 ** 31) *
         ((training_data['ST'] * 60) ** 0.91463667) *
         np.exp(-794349.13053 / (8.314 * (training_data['STT'] + 273.15)))) ** (1 / 2.15),
        # 计算 d_over_solution
        (training_data['IGS'] ** 0.91 + 2.4183382086666 * (10 ** 13) *
         ((training_data['ST'] * 60) ** 0.24630667) *
         np.exp(-339887.6589 / (8.314 * (training_data['STT'] + 273.15)))) ** (1 / 0.91)
    )

    # 创建一个只包含 d_solution 列的 DataFrame
    result_data = pd.DataFrame(d_solution, columns=['d_solution'])

    return result_data

def main():
    """Main"""
    ###Target
    '''Multiprocess'''
    pool1 = multiprocessing.Pool(processes=1)
    multi_list_grain_size = multiprocessing.Manager().list()
    map_list_grain_size = []
    for i in range(0, sampling_times):
        map_list_grain_size.append((training_data_grain_size, search_space, multi_list_grain_size))
    pool1.starmap(calculate_uncertainty_grainsize, map_list_grain_size)
    pool1.close()
    pool1.join()
    print_time()

    pool2 = multiprocessing.Pool(processes=1)
    multi_list_Primary_γ_size = multiprocessing.Manager().list()
    map_list_Primary_γ_size = []
    for i in range(0, sampling_times):
        map_list_Primary_γ_size.append((training_data_Primary_γ_size, search_space.loc[search_space['STT'] < 1083], multi_list_Primary_γ_size))
    pool2.starmap(calculate_uncertainty, map_list_Primary_γ_size)
    pool2.close()
    pool2.join()
    print_time()

    pool3 = multiprocessing.Pool(processes=1)
    multi_list_Primary_γ_volume_fraction = multiprocessing.Manager().list()
    map_list_Primary_γ_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_Primary_γ_volume_fraction.append((training_data_Primary_γ_volume_fraction, search_space.loc[search_space['STT'] < 1083], multi_list_Primary_γ_volume_fraction))
    pool3.starmap(calculate_uncertainty, map_list_Primary_γ_volume_fraction)
    pool3.close()
    pool3.join()
    print_time()

    pool4 = multiprocessing.Pool(processes=1)
    multi_list_Secondary_γ_size = multiprocessing.Manager().list()
    map_list_Secondary_γ_size = []
    for i in range(0, sampling_times):
        map_list_Secondary_γ_size.append((training_data_Secondary_γ_size, search_space.loc[search_space['STT'] < 1083], multi_list_Secondary_γ_size))
    pool4.starmap(calculate_uncertainty, map_list_Secondary_γ_size)
    pool4.close()
    pool4.join()
    print_time()

    pool5 = multiprocessing.Pool(processes=1)
    multi_list_Secondary_γ_volume_fraction = multiprocessing.Manager().list()
    map_list_Secondary_γ_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_Secondary_γ_volume_fraction.append((training_data_Secondary_γ_volume_fraction, search_space.loc[search_space['STT'] < 1083], multi_list_Secondary_γ_volume_fraction))
    pool5.starmap(calculate_uncertainty, map_list_Secondary_γ_volume_fraction)
    pool5.close()
    pool5.join()
    print_time()

    pool6 = multiprocessing.Pool(processes=1)
    multi_list_triplicate_γ_size = multiprocessing.Manager().list()
    map_list_triplicate_γ_size = []
    for i in range(0, sampling_times):
        map_list_triplicate_γ_size.append((training_data_triplicate_γ_size, search_space, multi_list_triplicate_γ_size))
    pool6.starmap(calculate_uncertainty, map_list_triplicate_γ_size)
    pool6.close()
    pool6.join()
    print_time()

    pool7 = multiprocessing.Pool(processes=1)
    multi_list_triplicate_γ_volume_fraction = multiprocessing.Manager().list()
    map_list_triplicate_γ_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_triplicate_γ_volume_fraction.append((training_data_triplicate_γ_volume_fraction, search_space, multi_list_triplicate_γ_volume_fraction))
    pool7.starmap(calculate_uncertainty, map_list_triplicate_γ_volume_fraction)
    pool7.close()
    pool7.join()
    print_time()

    pool8 = multiprocessing.Pool(processes=1)
    multi_list_yield_strength = multiprocessing.Manager().list()
    map_list_yield_strength = []
    for i in range(0, sampling_times):
        map_list_yield_strength.append((training_data_yield_strength, search_space, multi_list_yield_strength))
    pool8.starmap(calculate_uncertainty, map_list_yield_strength)
    pool8.close()
    pool8.join()
    print_time()

    predict_search_space_grain_size = pd.DataFrame(list(multi_list_grain_size))
    predict_search_space_Primary_γ_size = pd.DataFrame(list(multi_list_Primary_γ_size))
    predict_search_space_Primary_γ_volume_fraction = pd.DataFrame(list(multi_list_Primary_γ_volume_fraction))
    predict_search_space_Secondary_γ_size = pd.DataFrame(list(multi_list_Secondary_γ_size))
    predict_search_space_Secondary_γ_volume_fraction = pd.DataFrame(list(multi_list_Secondary_γ_volume_fraction))
    predict_search_space_triplicate_γ_size = pd.DataFrame(list(multi_list_triplicate_γ_size))
    predict_search_space_triplicate_γ_volume_fraction = pd.DataFrame(list(multi_list_triplicate_γ_volume_fraction))
    predict_search_space_yield_strength = pd.DataFrame(list(multi_list_yield_strength))

    # 屈服强度模型
    D = predict_search_space_grain_size.T
    f_Primary_γ = predict_search_space_Primary_γ_volume_fraction.T
    f_Secondary_γ = predict_search_space_Secondary_γ_volume_fraction.T
    f_triplicate_γ = predict_search_space_triplicate_γ_volume_fraction.T
    r_Primary_γ = predict_search_space_Primary_γ_size.T
    r_Secondary_γ = predict_search_space_Secondary_γ_size.T
    r_triplicate_γ = predict_search_space_triplicate_γ_size.T
    predict_search_space_yield_strength = predict_search_space_yield_strength.T
    b = 0.148 * 10 ** (-9)
    G = 96.94 * 10 ** (9)
    𝑦_𝐴𝑃𝐵 = 0.216

    D.to_csv('E:\experience profile\\predict_search_space_grain_size.csv', index=False)
    r_Primary_γ.to_csv('E:\experience profile\\predict_search_space_Primary_γ_size.csv',index=False)
    f_Primary_γ.to_csv('E:\experience profile\\predict_search_space_Primary_γ_volume_fraction.csv', index=False)
    r_Secondary_γ.to_csv('E:\experience profile\\predict_search_space_Secondary_γ_size.csv',index=False)
    f_Secondary_γ.to_csv('E:\experience profile\\predict_search_space_Secondary_γ_volume_fraction.csv', index=False)
    r_triplicate_γ.to_csv('E:\experience profile\\predict_search_space_triplicate_γ_size.csv',index=False)
    f_triplicate_γ.to_csv('E:\experience profile\\predict_search_space_triplicate_γ_volume_fraction.csv', index=False)
    predict_search_space_yield_strength.to_csv('E:\experience profile\\predict_search_space_yield_strength.csv',index=False)

    # 选出长度相同的部分
    length_same = len(f_Primary_γ)
    print(length_same)

    # 选择长度不匹配的部分(多出来的1083℃以上的部分)
    df_extra_f_triplicate_γ = f_triplicate_γ.iloc[length_same:]
    df_extra_r_triplicate_γ = r_triplicate_γ.iloc[length_same:]
    df_extra_D = D.iloc[length_same:]

    # 输出结果
    df_extra_f_triplicate_γ.to_csv('E:\experience profile\\df_extra_f_triplicate_γ.csv', index=False)
    df_extra_r_triplicate_γ.to_csv('E:\experience profile\\df_extra_r_triplicate_γ.csv', index=False)
    df_extra_D.to_csv('E:\experience profile\\df_extra_D.csv', index=False)

    # 选择长度匹配的部分(1083℃以下的部分)
    df_matched_f_triplicate_γ = f_triplicate_γ.iloc[:length_same]
    df_matched_r_triplicate_γ = r_triplicate_γ.iloc[:length_same]
    df_matched_D = D.iloc[:length_same]
    # 创建一个新的DataFrame

    # 输出结果
    df_matched_f_triplicate_γ.to_csv('E:\experience profile\\df_matched_f_triplicate_γ.csv', index=False)
    df_matched_r_triplicate_γ.to_csv('E:\experience profile\\df_matched_r_triplicate_γ.csv', index=False)
    df_matched_D.to_csv('E:\experience profile\\df_matched_D.csv', index=False)

    # 平均粒子间距
    L_Primary_γ = (((2 * math.pi) / (3 * f_Primary_γ))** 0.5) * r_Primary_γ
    L_Secondary_γ = (((2 * math.pi) / (3 * f_Secondary_γ)) ** 0.5) * r_Secondary_γ
    L_triplicate_γ_sub_solution = (((2 * math.pi) / (3 * df_matched_f_triplicate_γ)) ** 0.5) * df_matched_r_triplicate_γ
    L_triplicate_γ_over_solution = (((2 * math.pi) / (3 * df_extra_f_triplicate_γ)) ** 0.5) * df_extra_r_triplicate_γ

    # 位错线张力
    𝑇_𝐿 = 0.5 * 96.94 * (0.148**2) * (10 ** (-9))

    # 切割析出相粒子的位错有效长度
    𝜆_Secondary_γ = ((𝑇_𝐿 / (0.216 * r_Secondary_γ)) ** 0.5) * L_Secondary_γ
    𝜆_triplicate_γ_sub_solution = ((𝑇_𝐿 / (0.216 * df_matched_r_triplicate_γ)) ** 0.5) * L_triplicate_γ_sub_solution
    𝜆_triplicate_γ_over_solution = ((𝑇_𝐿 / (0.216 * df_extra_r_triplicate_γ)) ** 0.5) * L_triplicate_γ_over_solution

    # 屈服强度建模(亚固溶)
    # 晶界强化(𝜎_𝐷)
    𝜎_𝐷_sub_solution = 1813.71 / (df_matched_D ** 0.5)
    # γ中的固溶体强化
    𝜎_𝑠𝑠_sub_solution = 159.73 * (1 - f_Primary_γ - f_Secondary_γ - df_matched_f_triplicate_γ)
    # 奥罗万机制
    𝜎_Oro = (43.04 / (L_Primary_γ - 2 * r_Primary_γ))/ 10**6
    # 沉淀强化
    𝜎_𝑃_sub_solution = ((0.5 * ((G * b) / 𝜆_Secondary_γ) * (0.025 / math.pi) * (
                (2 * math.pi * 𝑦_𝐴𝑃𝐵 * r_Secondary_γ) / (0.0125 * G * (b ** 2)) - 1) ** 0.5) +
                        (0.5 * ((G * b) / 𝜆_triplicate_γ_sub_solution) * (0.0702 / math.pi) * (
                                    (2 * math.pi * 𝑦_𝐴𝑃𝐵 * df_matched_r_triplicate_γ) / (
                                        0.0351 * G * (b ** 2)) - 1) ** 0.5))/10**6

    𝝈_𝒀_sub_solution = 𝜎_𝐷_sub_solution + 𝜎_𝑠𝑠_sub_solution + (𝜎_Oro + 𝜎_𝑃_sub_solution) * 3.06

    # 屈服强度建模(过固溶)
    # 晶界强化(𝜎_𝐷)
    𝜎_𝐷_over_solution = 1804.34 / (df_extra_D ** 0.5)
    # γ中的固溶体强化
    𝜎_𝑠𝑠_over_solution = 159.73 * (1 - df_extra_f_triplicate_γ)
    # 沉淀强化
    𝜎_𝑃_over_solution = (0.5 * ((G * b) / 𝜆_triplicate_γ_over_solution) * (0.1 / math.pi) * (
                                    (2 * math.pi * 𝑦_𝐴𝑃𝐵 * df_extra_r_triplicate_γ) / (
                                        0.05 * G * (b ** 2)) - 1) ** 0.5)/10**6

    𝝈_𝒀_over_solution = 𝜎_𝐷_over_solution + 𝜎_𝑠𝑠_over_solution + 𝜎_𝑃_over_solution * 3.06

    # 创建一个新的DataFrame来存储结果
    yield_strengths = pd.concat([𝝈_𝒀_sub_solution, 𝝈_𝒀_over_solution], ignore_index=True)

    # 屈服强度预测数据保存到CSV文件
    print(yield_strengths)
    yield_strengths.to_csv('E:\experience profile\\yield_strengths.csv', index=False)


    # 调用函数
    grain_size_konwledge = grain_size(search_space)
    print(grain_size_konwledge)
    d_solution_columns = pd.concat([grain_size_konwledge] * sampling_times, axis=1)
    d_solution_columns.columns = range(sampling_times)
    print(d_solution_columns)

    # 晶粒尺寸模型重设索引
    d_solution_columns = d_solution_columns.reset_index(drop=True)
    D = D.reset_index(drop=True)

    # 进行相乘操作
    grain_size_multi = d_solution_columns.mul(D, axis=0)

    # 打印结果
    print(grain_size_multi)

    target = efficiency_function(predict_search_space_yield_strength.T, grain_size_multi.T,yield_strengths.T)

    print_time()

    #target = pd.DataFrame(target_pre[0])
    target_arr = target.nlargest(10,'ei_all')
    target_space = target_arr.index.values
    target_fin = search_space.iloc[target_space, :]
    target_plus = pd.concat([target_arr,target_fin],axis=1)
    print(target_plus)


    '''Output'''
    #target_name = 'EI_all'
    #file_name = target_name + '_for_EI1.csv'
    #target_plus.to_csv(path_or_buf=file_name, index=True)

    '''Output'''
    target_name = 'EI_all_moei'
    file_name = target_name + '_for_EI1.csv'
    target_plus.to_csv(path_or_buf=file_name, index=True)

if __name__ == '__main__':
    start = perf_counter()
    print("Start the EGO Program: ")

    '''Data preparation'''
    training_data_grain_size = pd.read_csv(open("E:\experience profile\\training_data_grain_size.csv", encoding='utf -8- sig'))
    training_data_Primary_γ_size = pd.read_csv(open("E:\experience profile\\training_data_Primary_γ_size.csv", encoding='utf -8- sig'))
    training_data_Primary_γ_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_Primary_γ_volume_fraction.xlsx.csv", encoding='utf -8- sig'))
    training_data_Secondary_γ_size = pd.read_csv(open("E:\experience profile\\training_data_Secondary_γ_size.csv", encoding='utf -8- sig'))
    training_data_Secondary_γ_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_Secondary_γ_volume_fraction.csv", encoding='utf -8- sig'))
    training_data_triplicate_γ_size = pd.read_csv(open("E:\experience profile\\training_data_triplicate_γ_size.csv", encoding='utf -8- sig'))
    training_data_triplicate_γ_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_triplicate_γ_volume_fraction.csv", encoding='utf -8- sig'))
    training_data_yield_strength = pd.read_csv(open("E:\experience profile\\training_data_yield_strength.csv", encoding='utf -8- sig'))

    #搜索空间
    search_space = pd.read_csv(open("E:\experience profile\\search_space5.csv", encoding='utf -8- sig'))

    sampling_times =500






    '''Main'''
    main()

    end = perf_counter()
    print("The program took " + str(end - start) + " seconds")
