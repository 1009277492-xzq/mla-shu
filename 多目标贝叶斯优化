"""
EGO
"""

'''Loading package'''
import pandas as pd
import numpy as np
from itertools import combinations
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from time import strftime
from time import localtime
from time import perf_counter
from time import sleep
from tqdm import tqdm
import math
import scipy.stats as st
import multiprocessing
from itertools import product
import random

random.seed(654321)
np.random.seed(654321)

'''A function that returns the current time'''
def print_time():
    print(strftime("%Y-%m-%d %H:%M:%S", localtime()))
    return


'''A function that returns the percentage error'''
def percentage_error(y, y_predict ):
    error = np.average( abs(np.subtract(y_predict, y) / y))
    return error * 100
EP_scorer = make_scorer(percentage_error, greater_is_better = False)
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
def calculate_uncertainty(training, search, global_list):
    # Define the parameter grid
    param_grid = {
        'svr__C': [1, 10, 30, 60, 80, 100, 150, 200],
        'svr__gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2]
    }

    number_data = round(training.shape[0] * 0.9)
    sub_training_data = training.sample(number_data, replace=False)

    # Extract features and labels from the training set
    X_train = sub_training_data.iloc[:, :-1].values
    y_train = sub_training_data.iloc[:, -1].values

    # Standardize the features and normalize the labels
    scaler_features = StandardScaler()
    scaler_labels = MinMaxScaler()  # Normalize the labels to be in the range [0, 1]

    X_train_scaled = scaler_features.fit_transform(X_train)
    y_train_normalized = scaler_labels.fit_transform(y_train.reshape(-1, 1)).flatten()

    # Create a pipeline with scaling and SVR
    svr = make_pipeline(StandardScaler(), SVR(kernel='rbf'))

    # Initialize GridSearchCV with the scaled data
    grids = GridSearchCV(svr, param_grid, cv=10, scoring=mse_scorer, n_jobs=1)

    # Fit the model on the scaled data
    grids.fit(X_train_scaled, y_train_normalized)

    # Get the best estimator
    best_clf = grids.best_estimator_

    # Scale the test data
    X_test = search.iloc[:, :].values
    X_test_scaled = scaler_features.transform(X_test)

    # Make predictions on the test data
    predict_test_normalized = best_clf.predict(X_test_scaled)

    # Inverse transform the predictions back to original scale
    predict_test = scaler_labels.inverse_transform(predict_test_normalized.reshape(-1, 1)).flatten()

    # Append the predictions to the global list
    global_list.append(predict_test)
    print_time()

def calculate_uncertainty_grainsize(training, search, global_list):
    # Define the parameter grid
    param_grid = {
        'svr__C': [1, 10, 30, 60, 80, 100, 150, 200],
        'svr__gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2]
    }

    number_data = round(training.shape[0] * 0.9)
    sub_training_data = training.sample(number_data, replace=False)

    # Extract features and labels from the training set
    X_train = sub_training_data.iloc[:, :-1].values
    y_train = sub_training_data.iloc[:, -1].values

    # Standardize the features and normalize the labels
    scaler_features = StandardScaler()
    scaler_labels = MinMaxScaler()  # Normalize the labels to be in the range [0, 1]

    X_train_scaled = scaler_features.fit_transform(X_train)
    y_train_normalized = scaler_labels.fit_transform(y_train.reshape(-1, 1)).flatten()

    # Create a pipeline with scaling and SVR
    svr = make_pipeline(StandardScaler(), SVR(kernel='rbf'))

    # Initialize GridSearchCV with the scaled data
    grids = GridSearchCV(svr, param_grid, cv=10, scoring=mse_scorer, n_jobs=1)

    # Fit the model on the scaled data
    grids.fit(X_train_scaled, y_train_normalized)

    # Get the best estimator
    best_clf = grids.best_estimator_

    # Scale the test data
    X_test = search.iloc[:, :].values
    X_test_scaled = scaler_features.transform(X_test)

    # Make predictions on the test data
    predict_test_normalized = best_clf.predict(X_test_scaled)

    # Inverse transform the predictions back to original scale
    predict_test = scaler_labels.inverse_transform(predict_test_normalized.reshape(-1, 1)).flatten()

    # Ensure all predictions are above 27
    predict_test = np.array([max(val, 27) for val in predict_test])

    # Append the predictions to the global list
    global_list.append(predict_test)


def features_calculation():
    pass

def efficiency_function(search_space_prediction_yield_stength,knowledge_prediction_grain_size,search_space_knoledge_yield_stength):
    bench_mark_search_space_prediction_yield_stength = 987
    bench_mark_knowledge_prediction_grain_size = -30*30
    bench_mark_search_space_knoledge_yield_stength = 994

    search_space_prediction_yield_stength = search_space_prediction_yield_stength.describe().iloc[1:3, :]
    search_space_prediction_yield_stength = pd.DataFrame(search_space_prediction_yield_stength.values.T, index=search_space_prediction_yield_stength.columns, columns=search_space_prediction_yield_stength.index)
    average_value_search_space_prediction_yield_stength = search_space_prediction_yield_stength.iloc[:,0]
    standard_deviation_search_space_prediction_yield_stength = search_space_prediction_yield_stength.iloc[:,1]

    knowledge_prediction_grain_size = (-knowledge_prediction_grain_size).describe().iloc[1:3, :]
    knowledge_prediction_grain_size = pd.DataFrame(knowledge_prediction_grain_size.values.T, index=knowledge_prediction_grain_size.columns,columns=knowledge_prediction_grain_size.index)
    average_value_knowledge_prediction_grain_size = knowledge_prediction_grain_size.iloc[:, 0]
    standard_deviation_knowledge_prediction_grain_size = knowledge_prediction_grain_size.iloc[:, 1]

    search_space_knoledge_yield_stength = (search_space_knoledge_yield_stength).describe().iloc[1:3, :]
    search_space_knoledge_yield_stength = pd.DataFrame(search_space_knoledge_yield_stength.values.T,index=search_space_knoledge_yield_stength.columns,columns=search_space_knoledge_yield_stength.index)
    average_value_knowledge_search_space_knoledge_yield_stength = search_space_knoledge_yield_stength.iloc[:, 0]
    standard_deviation_search_space_knoledge_yield_stength = search_space_knoledge_yield_stength.iloc[:, 1]

    #EGO
    z_ego_search_space_prediction_yield_stength = np.true_divide(np.subtract(average_value_search_space_prediction_yield_stength, bench_mark_search_space_prediction_yield_stength), standard_deviation_search_space_prediction_yield_stength)
    ei_ego_search_space_prediction_yield_stength = standard_deviation_search_space_prediction_yield_stength * z_ego_search_space_prediction_yield_stength * st.norm.cdf(z_ego_search_space_prediction_yield_stength) + standard_deviation_search_space_prediction_yield_stength * st.norm.pdf(z_ego_search_space_prediction_yield_stength)

    z_ego_knowledge_prediction_grain_size = np.true_divide(np.subtract(average_value_knowledge_prediction_grain_size, bench_mark_knowledge_prediction_grain_size), standard_deviation_knowledge_prediction_grain_size)
    ei_ego_knowledge_prediction_grain_size = standard_deviation_knowledge_prediction_grain_size * z_ego_knowledge_prediction_grain_size * st.norm.cdf(z_ego_knowledge_prediction_grain_size) + standard_deviation_knowledge_prediction_grain_size * st.norm.pdf(z_ego_knowledge_prediction_grain_size)

    z_ego_search_space_knoledge_yield_stength = np.true_divide(np.subtract(average_value_knowledge_search_space_knoledge_yield_stength, bench_mark_search_space_knoledge_yield_stength), standard_deviation_search_space_knoledge_yield_stength)
    ei_ego_search_space_knoledge_yield_stength = standard_deviation_search_space_knoledge_yield_stength * z_ego_search_space_knoledge_yield_stength * st.norm.cdf(z_ego_search_space_knoledge_yield_stength) + standard_deviation_search_space_knoledge_yield_stength * st.norm.pdf(z_ego_search_space_knoledge_yield_stength)

    #KG
    #z_kg_search_space_prediction_yield_stength = -1*abs(np.true_divide(np.subtract(average_value_search_space_prediction_yield_stength,max(max(average_value_search_space_prediction_yield_stength),bench_mark_search_space_prediction_yield_stength)),standard_deviation_search_space_prediction_yield_stength))
    #ei_kg_search_space_prediction_yield_stength = standard_deviation_search_space_prediction_yield_stength * z_kg_search_space_prediction_yield_stength * st.norm.cdf(z_kg_search_space_prediction_yield_stength) + standard_deviation_search_space_prediction_yield_stength * st.norm.pdf(z_kg_search_space_prediction_yield_stength)

    #z_kg_knowledge_prediction_grain_size = -1 * abs(np.true_divide(np.subtract(average_value_knowledge_prediction_grain_size,max(max(average_value_knowledge_prediction_grain_size),bench_mark_knowledge_prediction_grain_size)),standard_deviation_knowledge_prediction_grain_size))
    #ei_kg_knowledge_prediction_grain_size = standard_deviation_knowledge_prediction_grain_size * z_kg_knowledge_prediction_grain_size * st.norm.cdf(z_kg_knowledge_prediction_grain_size) + standard_deviation_knowledge_prediction_grain_size * st.norm.pdf(z_kg_knowledge_prediction_grain_size)

    #z_kg_search_space_knoledge_yield_stength = -1 * abs(np.true_divide(np.subtract(average_value_knowledge_search_space_knoledge_yield_stength,max(max(average_value_knowledge_search_space_knoledge_yield_stength),bench_mark_search_space_knoledge_yield_stength)),standard_deviation_search_space_knoledge_yield_stength))
    #ei_kg_search_space_knoledge_yield_stength = standard_deviation_search_space_knoledge_yield_stength * z_kg_search_space_knoledge_yield_stength * st.norm.cdf(z_kg_search_space_knoledge_yield_stength) + standard_deviation_search_space_knoledge_yield_stength * st.norm.pdf(z_kg_search_space_knoledge_yield_stength)

    #Max_P
    #max_p = st.norm.cdf(z_ego,average_value,standard_deviation)


    #ei_all  = ei_ego_search_space_prediction_yield_stength.mul(ei_ego_knowledge_prediction_grain_size).mul(ei_ego_search_space_knoledge_yield_stength)
    #ei = search_space_prediction_yield_stength
    #ei['ei_all'] = ei_all

    ei_all  = ei_ego_search_space_prediction_yield_stength.mul(ei_ego_knowledge_prediction_grain_size).mul(ei_ego_search_space_knoledge_yield_stength)
    ei = search_space_prediction_yield_stength
    ei['ei_all'] = ei_all

    #ei['ei_kg'] = ei_kg
    #ei['max_p'] = max_p
    print_time()
    return ei


def grain_size(training_data):
    # å®šä¹‰é˜ˆå€¼
    threshold = 1083

    d_solution = np.where(
        training_data['STT'] < threshold,
        # è®¡ç®— d_Sub_solution
        (training_data['IGS'] ** 2.15 + 1.027584834 * (10 ** 31) *
         ((training_data['ST'] * 60) ** 0.91463667) *
         np.exp(-794349.13053 / (8.314 * (training_data['STT'] + 273.15)))) ** (1 / 2.15),
        # è®¡ç®— d_over_solution
        (training_data['IGS'] ** 0.91 + 2.4183382086666 * (10 ** 13) *
         ((training_data['ST'] * 60) ** 0.24630667) *
         np.exp(-339887.6589 / (8.314 * (training_data['STT'] + 273.15)))) ** (1 / 0.91)
    )

    # åˆ›å»ºä¸€ä¸ªåªåŒ…å« d_solution åˆ—çš„ DataFrame
    result_data = pd.DataFrame(d_solution, columns=['d_solution'])

    return result_data

def main():
    """Main"""
    ###Target
    '''Multiprocess'''
    pool1 = multiprocessing.Pool(processes=1)
    multi_list_grain_size = multiprocessing.Manager().list()
    map_list_grain_size = []
    for i in range(0, sampling_times):
        map_list_grain_size.append((training_data_grain_size, search_space, multi_list_grain_size))
    pool1.starmap(calculate_uncertainty_grainsize, map_list_grain_size)
    pool1.close()
    pool1.join()
    print_time()

    pool2 = multiprocessing.Pool(processes=1)
    multi_list_Primary_Î³_size = multiprocessing.Manager().list()
    map_list_Primary_Î³_size = []
    for i in range(0, sampling_times):
        map_list_Primary_Î³_size.append((training_data_Primary_Î³_size, search_space.loc[search_space['STT'] < 1083], multi_list_Primary_Î³_size))
    pool2.starmap(calculate_uncertainty, map_list_Primary_Î³_size)
    pool2.close()
    pool2.join()
    print_time()

    pool3 = multiprocessing.Pool(processes=1)
    multi_list_Primary_Î³_volume_fraction = multiprocessing.Manager().list()
    map_list_Primary_Î³_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_Primary_Î³_volume_fraction.append((training_data_Primary_Î³_volume_fraction, search_space.loc[search_space['STT'] < 1083], multi_list_Primary_Î³_volume_fraction))
    pool3.starmap(calculate_uncertainty, map_list_Primary_Î³_volume_fraction)
    pool3.close()
    pool3.join()
    print_time()

    pool4 = multiprocessing.Pool(processes=1)
    multi_list_Secondary_Î³_size = multiprocessing.Manager().list()
    map_list_Secondary_Î³_size = []
    for i in range(0, sampling_times):
        map_list_Secondary_Î³_size.append((training_data_Secondary_Î³_size, search_space.loc[search_space['STT'] < 1083], multi_list_Secondary_Î³_size))
    pool4.starmap(calculate_uncertainty, map_list_Secondary_Î³_size)
    pool4.close()
    pool4.join()
    print_time()

    pool5 = multiprocessing.Pool(processes=1)
    multi_list_Secondary_Î³_volume_fraction = multiprocessing.Manager().list()
    map_list_Secondary_Î³_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_Secondary_Î³_volume_fraction.append((training_data_Secondary_Î³_volume_fraction, search_space.loc[search_space['STT'] < 1083], multi_list_Secondary_Î³_volume_fraction))
    pool5.starmap(calculate_uncertainty, map_list_Secondary_Î³_volume_fraction)
    pool5.close()
    pool5.join()
    print_time()

    pool6 = multiprocessing.Pool(processes=1)
    multi_list_triplicate_Î³_size = multiprocessing.Manager().list()
    map_list_triplicate_Î³_size = []
    for i in range(0, sampling_times):
        map_list_triplicate_Î³_size.append((training_data_triplicate_Î³_size, search_space, multi_list_triplicate_Î³_size))
    pool6.starmap(calculate_uncertainty, map_list_triplicate_Î³_size)
    pool6.close()
    pool6.join()
    print_time()

    pool7 = multiprocessing.Pool(processes=1)
    multi_list_triplicate_Î³_volume_fraction = multiprocessing.Manager().list()
    map_list_triplicate_Î³_volume_fraction = []
    for i in range(0, sampling_times):
        map_list_triplicate_Î³_volume_fraction.append((training_data_triplicate_Î³_volume_fraction, search_space, multi_list_triplicate_Î³_volume_fraction))
    pool7.starmap(calculate_uncertainty, map_list_triplicate_Î³_volume_fraction)
    pool7.close()
    pool7.join()
    print_time()

    pool8 = multiprocessing.Pool(processes=1)
    multi_list_yield_strength = multiprocessing.Manager().list()
    map_list_yield_strength = []
    for i in range(0, sampling_times):
        map_list_yield_strength.append((training_data_yield_strength, search_space, multi_list_yield_strength))
    pool8.starmap(calculate_uncertainty, map_list_yield_strength)
    pool8.close()
    pool8.join()
    print_time()

    predict_search_space_grain_size = pd.DataFrame(list(multi_list_grain_size))
    predict_search_space_Primary_Î³_size = pd.DataFrame(list(multi_list_Primary_Î³_size))
    predict_search_space_Primary_Î³_volume_fraction = pd.DataFrame(list(multi_list_Primary_Î³_volume_fraction))
    predict_search_space_Secondary_Î³_size = pd.DataFrame(list(multi_list_Secondary_Î³_size))
    predict_search_space_Secondary_Î³_volume_fraction = pd.DataFrame(list(multi_list_Secondary_Î³_volume_fraction))
    predict_search_space_triplicate_Î³_size = pd.DataFrame(list(multi_list_triplicate_Î³_size))
    predict_search_space_triplicate_Î³_volume_fraction = pd.DataFrame(list(multi_list_triplicate_Î³_volume_fraction))
    predict_search_space_yield_strength = pd.DataFrame(list(multi_list_yield_strength))

    # å±ˆæœå¼ºåº¦æ¨¡åž‹
    D = predict_search_space_grain_size.T
    f_Primary_Î³ = predict_search_space_Primary_Î³_volume_fraction.T
    f_Secondary_Î³ = predict_search_space_Secondary_Î³_volume_fraction.T
    f_triplicate_Î³ = predict_search_space_triplicate_Î³_volume_fraction.T
    r_Primary_Î³ = predict_search_space_Primary_Î³_size.T
    r_Secondary_Î³ = predict_search_space_Secondary_Î³_size.T
    r_triplicate_Î³ = predict_search_space_triplicate_Î³_size.T
    predict_search_space_yield_strength = predict_search_space_yield_strength.T
    b = 0.148 * 10 ** (-9)
    G = 96.94 * 10 ** (9)
    ð‘¦_ð´ð‘ƒðµ = 0.216

    D.to_csv('E:\experience profile\\predict_search_space_grain_size.csv', index=False)
    r_Primary_Î³.to_csv('E:\experience profile\\predict_search_space_Primary_Î³_size.csv',index=False)
    f_Primary_Î³.to_csv('E:\experience profile\\predict_search_space_Primary_Î³_volume_fraction.csv', index=False)
    r_Secondary_Î³.to_csv('E:\experience profile\\predict_search_space_Secondary_Î³_size.csv',index=False)
    f_Secondary_Î³.to_csv('E:\experience profile\\predict_search_space_Secondary_Î³_volume_fraction.csv', index=False)
    r_triplicate_Î³.to_csv('E:\experience profile\\predict_search_space_triplicate_Î³_size.csv',index=False)
    f_triplicate_Î³.to_csv('E:\experience profile\\predict_search_space_triplicate_Î³_volume_fraction.csv', index=False)
    predict_search_space_yield_strength.to_csv('E:\experience profile\\predict_search_space_yield_strength.csv',index=False)

    # é€‰å‡ºé•¿åº¦ç›¸åŒçš„éƒ¨åˆ†
    length_same = len(f_Primary_Î³)
    print(length_same)

    # é€‰æ‹©é•¿åº¦ä¸åŒ¹é…çš„éƒ¨åˆ†(å¤šå‡ºæ¥çš„1083â„ƒä»¥ä¸Šçš„éƒ¨åˆ†)
    df_extra_f_triplicate_Î³ = f_triplicate_Î³.iloc[length_same:]
    df_extra_r_triplicate_Î³ = r_triplicate_Î³.iloc[length_same:]
    df_extra_D = D.iloc[length_same:]

    # è¾“å‡ºç»“æžœ
    df_extra_f_triplicate_Î³.to_csv('E:\experience profile\\df_extra_f_triplicate_Î³.csv', index=False)
    df_extra_r_triplicate_Î³.to_csv('E:\experience profile\\df_extra_r_triplicate_Î³.csv', index=False)
    df_extra_D.to_csv('E:\experience profile\\df_extra_D.csv', index=False)

    # é€‰æ‹©é•¿åº¦åŒ¹é…çš„éƒ¨åˆ†(1083â„ƒä»¥ä¸‹çš„éƒ¨åˆ†)
    df_matched_f_triplicate_Î³ = f_triplicate_Î³.iloc[:length_same]
    df_matched_r_triplicate_Î³ = r_triplicate_Î³.iloc[:length_same]
    df_matched_D = D.iloc[:length_same]
    # åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrame

    # è¾“å‡ºç»“æžœ
    df_matched_f_triplicate_Î³.to_csv('E:\experience profile\\df_matched_f_triplicate_Î³.csv', index=False)
    df_matched_r_triplicate_Î³.to_csv('E:\experience profile\\df_matched_r_triplicate_Î³.csv', index=False)
    df_matched_D.to_csv('E:\experience profile\\df_matched_D.csv', index=False)

    # å¹³å‡ç²’å­é—´è·
    L_Primary_Î³ = (((2 * math.pi) / (3 * f_Primary_Î³))** 0.5) * r_Primary_Î³
    L_Secondary_Î³ = (((2 * math.pi) / (3 * f_Secondary_Î³)) ** 0.5) * r_Secondary_Î³
    L_triplicate_Î³_sub_solution = (((2 * math.pi) / (3 * df_matched_f_triplicate_Î³)) ** 0.5) * df_matched_r_triplicate_Î³
    L_triplicate_Î³_over_solution = (((2 * math.pi) / (3 * df_extra_f_triplicate_Î³)) ** 0.5) * df_extra_r_triplicate_Î³

    # ä½é”™çº¿å¼ åŠ›
    ð‘‡_ð¿ = 0.5 * 96.94 * (0.148**2) * (10 ** (-9))

    # åˆ‡å‰²æžå‡ºç›¸ç²’å­çš„ä½é”™æœ‰æ•ˆé•¿åº¦
    ðœ†_Secondary_Î³ = ((ð‘‡_ð¿ / (0.216 * r_Secondary_Î³)) ** 0.5) * L_Secondary_Î³
    ðœ†_triplicate_Î³_sub_solution = ((ð‘‡_ð¿ / (0.216 * df_matched_r_triplicate_Î³)) ** 0.5) * L_triplicate_Î³_sub_solution
    ðœ†_triplicate_Î³_over_solution = ((ð‘‡_ð¿ / (0.216 * df_extra_r_triplicate_Î³)) ** 0.5) * L_triplicate_Î³_over_solution

    # å±ˆæœå¼ºåº¦å»ºæ¨¡(äºšå›ºæº¶)
    # æ™¶ç•Œå¼ºåŒ–(ðœŽ_ð·)
    ðœŽ_ð·_sub_solution = 1813.71 / (df_matched_D ** 0.5)
    # Î³ä¸­çš„å›ºæº¶ä½“å¼ºåŒ–
    ðœŽ_ð‘ ð‘ _sub_solution = 159.73 * (1 - f_Primary_Î³ - f_Secondary_Î³ - df_matched_f_triplicate_Î³)
    # å¥¥ç½—ä¸‡æœºåˆ¶
    ðœŽ_Oro = (43.04 / (L_Primary_Î³ - 2 * r_Primary_Î³))/ 10**6
    # æ²‰æ·€å¼ºåŒ–
    ðœŽ_ð‘ƒ_sub_solution = ((0.5 * ((G * b) / ðœ†_Secondary_Î³) * (0.025 / math.pi) * (
                (2 * math.pi * ð‘¦_ð´ð‘ƒðµ * r_Secondary_Î³) / (0.0125 * G * (b ** 2)) - 1) ** 0.5) +
                        (0.5 * ((G * b) / ðœ†_triplicate_Î³_sub_solution) * (0.0702 / math.pi) * (
                                    (2 * math.pi * ð‘¦_ð´ð‘ƒðµ * df_matched_r_triplicate_Î³) / (
                                        0.0351 * G * (b ** 2)) - 1) ** 0.5))/10**6

    ðˆ_ð’€_sub_solution = ðœŽ_ð·_sub_solution + ðœŽ_ð‘ ð‘ _sub_solution + (ðœŽ_Oro + ðœŽ_ð‘ƒ_sub_solution) * 3.06

    # å±ˆæœå¼ºåº¦å»ºæ¨¡(è¿‡å›ºæº¶)
    # æ™¶ç•Œå¼ºåŒ–(ðœŽ_ð·)
    ðœŽ_ð·_over_solution = 1804.34 / (df_extra_D ** 0.5)
    # Î³ä¸­çš„å›ºæº¶ä½“å¼ºåŒ–
    ðœŽ_ð‘ ð‘ _over_solution = 159.73 * (1 - df_extra_f_triplicate_Î³)
    # æ²‰æ·€å¼ºåŒ–
    ðœŽ_ð‘ƒ_over_solution = (0.5 * ((G * b) / ðœ†_triplicate_Î³_over_solution) * (0.1 / math.pi) * (
                                    (2 * math.pi * ð‘¦_ð´ð‘ƒðµ * df_extra_r_triplicate_Î³) / (
                                        0.05 * G * (b ** 2)) - 1) ** 0.5)/10**6

    ðˆ_ð’€_over_solution = ðœŽ_ð·_over_solution + ðœŽ_ð‘ ð‘ _over_solution + ðœŽ_ð‘ƒ_over_solution * 3.06

    # åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameæ¥å­˜å‚¨ç»“æžœ
    yield_strengths = pd.concat([ðˆ_ð’€_sub_solution, ðˆ_ð’€_over_solution], ignore_index=True)

    # å±ˆæœå¼ºåº¦é¢„æµ‹æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶
    print(yield_strengths)
    yield_strengths.to_csv('E:\experience profile\\yield_strengths.csv', index=False)


    # è°ƒç”¨å‡½æ•°
    grain_size_konwledge = grain_size(search_space)
    print(grain_size_konwledge)
    d_solution_columns = pd.concat([grain_size_konwledge] * sampling_times, axis=1)
    d_solution_columns.columns = range(sampling_times)
    print(d_solution_columns)

    # æ™¶ç²’å°ºå¯¸æ¨¡åž‹é‡è®¾ç´¢å¼•
    d_solution_columns = d_solution_columns.reset_index(drop=True)
    D = D.reset_index(drop=True)

    # è¿›è¡Œç›¸ä¹˜æ“ä½œ
    grain_size_multi = d_solution_columns.mul(D, axis=0)

    # æ‰“å°ç»“æžœ
    print(grain_size_multi)

    target = efficiency_function(predict_search_space_yield_strength.T, grain_size_multi.T,yield_strengths.T)

    print_time()

    #target = pd.DataFrame(target_pre[0])
    target_arr = target.nlargest(10,'ei_all')
    target_space = target_arr.index.values
    target_fin = search_space.iloc[target_space, :]
    target_plus = pd.concat([target_arr,target_fin],axis=1)
    print(target_plus)


    '''Output'''
    #target_name = 'EI_all'
    #file_name = target_name + '_for_EI1.csv'
    #target_plus.to_csv(path_or_buf=file_name, index=True)

    '''Output'''
    target_name = 'EI_all_moei'
    file_name = target_name + '_for_EI1.csv'
    target_plus.to_csv(path_or_buf=file_name, index=True)

if __name__ == '__main__':
    start = perf_counter()
    print("Start the EGO Program: ")

    '''Data preparation'''
    training_data_grain_size = pd.read_csv(open("E:\experience profile\\training_data_grain_size.csv", encoding='utf -8- sig'))
    training_data_Primary_Î³_size = pd.read_csv(open("E:\experience profile\\training_data_Primary_Î³_size.csv", encoding='utf -8- sig'))
    training_data_Primary_Î³_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_Primary_Î³_volume_fraction.xlsx.csv", encoding='utf -8- sig'))
    training_data_Secondary_Î³_size = pd.read_csv(open("E:\experience profile\\training_data_Secondary_Î³_size.csv", encoding='utf -8- sig'))
    training_data_Secondary_Î³_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_Secondary_Î³_volume_fraction.csv", encoding='utf -8- sig'))
    training_data_triplicate_Î³_size = pd.read_csv(open("E:\experience profile\\training_data_triplicate_Î³_size.csv", encoding='utf -8- sig'))
    training_data_triplicate_Î³_volume_fraction = pd.read_csv(
        open("E:\experience profile\\training_data_triplicate_Î³_volume_fraction.csv", encoding='utf -8- sig'))
    training_data_yield_strength = pd.read_csv(open("E:\experience profile\\training_data_yield_strength.csv", encoding='utf -8- sig'))

    #æœç´¢ç©ºé—´
    search_space = pd.read_csv(open("E:\experience profile\\search_space5.csv", encoding='utf -8- sig'))

    sampling_times =500






    '''Main'''
    main()

    end = perf_counter()
    print("The program took " + str(end - start) + " seconds")
